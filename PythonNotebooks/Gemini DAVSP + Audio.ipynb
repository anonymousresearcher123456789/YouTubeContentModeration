{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e910bf",
   "metadata": {
    "id": "exmY-abTefFW",
    "papermill": {
     "duration": 0.007855,
     "end_time": "2025-07-29T02:12:13.848084",
     "exception": false,
     "start_time": "2025-07-29T02:12:13.840229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb29ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:12:13.863662Z",
     "iopub.status.busy": "2025-07-29T02:12:13.863234Z",
     "iopub.status.idle": "2025-07-29T02:16:43.647638Z",
     "shell.execute_reply": "2025-07-29T02:16:43.646131Z"
    },
    "id": "Bi3c0wLhh-Rf",
    "outputId": "44c463e5-7c4c-45cf-daca-75620249739c",
    "papermill": {
     "duration": 269.795425,
     "end_time": "2025-07-29T02:16:43.650507",
     "exception": false,
     "start_time": "2025-07-29T02:12:13.855082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install av\n",
    "!pip install scenedetect\n",
    "!pip install ffmpeg-python\n",
    "!pip install --upgrade pip\n",
    "!pip install -q -U google-generativeai\n",
    "!pip install --upgrade transformers datasets[audio] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14286d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:43.791622Z",
     "iopub.status.busy": "2025-07-29T02:16:43.791224Z",
     "iopub.status.idle": "2025-07-29T02:16:49.858032Z",
     "shell.execute_reply": "2025-07-29T02:16:49.857065Z"
    },
    "papermill": {
     "duration": 6.13636,
     "end_time": "2025-07-29T02:16:49.860542",
     "exception": false,
     "start_time": "2025-07-29T02:16:43.724182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import ffmpeg\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import soundfile as sf\n",
    "from json import loads,dumps\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import resample\n",
    "import typing_extensions as typing\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from scenedetect import open_video, VideoStreamCv2, SceneManager\n",
    "from scenedetect.detectors import ContentDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579e920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:49.991883Z",
     "iopub.status.busy": "2025-07-29T02:16:49.991318Z",
     "iopub.status.idle": "2025-07-29T02:16:50.919146Z",
     "shell.execute_reply": "2025-07-29T02:16:50.918118Z"
    },
    "id": "1ksUpR0ibKve",
    "outputId": "6880ee21-f703-4761-dfcd-842e1f8b61d9",
    "papermill": {
     "duration": 0.995982,
     "end_time": "2025-07-29T02:16:50.921587",
     "exception": false,
     "start_time": "2025-07-29T02:16:49.925605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting all required ids \n",
    "\n",
    "df = pd.read_csv('/kaggle/input/youtube-data/ground_labels_new.csv') \n",
    "\n",
    "df['Primary Label'] = df['Primary Label'].str.lower() \n",
    "df = df[df['Primary Label'].isin(['appropriate', 'inappropriate'])]\n",
    "\n",
    "ids_df = pd.read_csv('/kaggle/input/filtered-ground-truth/filtered_ground_truth.csv') \n",
    "ids_df['Video Id'] = ids_df['Video Id'].str.lstrip(\"'\") \n",
    "ids_ = ids_df['Video Id'].to_list() \n",
    "df = df[df['Video Id'].isin(ids_)] \n",
    "\n",
    "transcriptions_df = pd.read_csv('/kaggle/input/youtube-data/eng-complete-transcriptions.csv')\n",
    "df = pd.merge(df, transcriptions_df, on='Video Id', how='inner') \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c15b61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:51.051888Z",
     "iopub.status.busy": "2025-07-29T02:16:51.051530Z",
     "iopub.status.idle": "2025-07-29T02:16:51.057892Z",
     "shell.execute_reply": "2025-07-29T02:16:51.056838Z"
    },
    "id": "oR7rNEX3rl2I",
    "papermill": {
     "duration": 0.074369,
     "end_time": "2025-07-29T02:16:51.060000",
     "exception": false,
     "start_time": "2025-07-29T02:16:50.985631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting video ids and primary labels\n",
    "\n",
    "video_ids = list(df['Video Id'])\n",
    "primary_labels = list(df['Primary Label'])\n",
    "all_transcriptions = list(df['Transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8475979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:51.246713Z",
     "iopub.status.busy": "2025-07-29T02:16:51.246340Z",
     "iopub.status.idle": "2025-07-29T02:16:51.266767Z",
     "shell.execute_reply": "2025-07-29T02:16:51.265626Z"
    },
    "id": "_7p-efT9sNMf",
    "papermill": {
     "duration": 0.144088,
     "end_time": "2025-07-29T02:16:51.268767",
     "exception": false,
     "start_time": "2025-07-29T02:16:51.124679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting data from transcripts\n",
    "\n",
    "transcriptions = []\n",
    "lengths = []\n",
    "\n",
    "for (i, id_) in enumerate(video_ids):\n",
    "    transcriptions.append(all_transcriptions[i].split(\"chunks\")[0])\n",
    "    lengths.append(len(all_transcriptions[i].split(\"chunks\")[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68fe16f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:51.401507Z",
     "iopub.status.busy": "2025-07-29T02:16:51.401134Z",
     "iopub.status.idle": "2025-07-29T02:16:51.438140Z",
     "shell.execute_reply": "2025-07-29T02:16:51.436944Z"
    },
    "papermill": {
     "duration": 0.106241,
     "end_time": "2025-07-29T02:16:51.440343",
     "exception": false,
     "start_time": "2025-07-29T02:16:51.334102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "available_ids = os.listdir('/kaggle/input/youtube-data/Ads/Ads') \n",
    "len(available_ids) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503518b",
   "metadata": {
    "papermill": {
     "duration": 0.065106,
     "end_time": "2025-07-29T02:16:51.571427",
     "exception": false,
     "start_time": "2025-07-29T02:16:51.506321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Extracting Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24faca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:51.704630Z",
     "iopub.status.busy": "2025-07-29T02:16:51.704158Z",
     "iopub.status.idle": "2025-07-29T02:16:51.718489Z",
     "shell.execute_reply": "2025-07-29T02:16:51.717418Z"
    },
    "papermill": {
     "duration": 0.083876,
     "end_time": "2025-07-29T02:16:51.720860",
     "exception": false,
     "start_time": "2025-07-29T02:16:51.636984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_scenes(video_path, threshold = 30):\n",
    "    \"\"\"Detect scenes in a video and return scene start and end frames.\"\"\"\n",
    "    scene_list = []\n",
    "    while len(scene_list) < 6 and threshold > 0:\n",
    "        threshold //= 2\n",
    "    \n",
    "        video = open_video(video_path)\n",
    "        scene_manager = SceneManager()\n",
    "        scene_manager.add_detector(ContentDetector(threshold=threshold))\n",
    "    \n",
    "        scene_manager.detect_scenes(video)\n",
    "        scene_list = scene_manager.get_scene_list()\n",
    "    \n",
    "    return scene_list\n",
    "\n",
    "\n",
    "def get_top_n_longest_scenes(scene_list, n):\n",
    "    '''Return the top n longest scenes with start and end frame indices.'''\n",
    "    scene_durations = [(start, end - start) for start, end in scene_list]\n",
    "    scene_durations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Top n longest scenes with start and end frame indices\n",
    "    longest_scenes = [(start, start + duration) for start, duration in scene_durations[:n]]\n",
    "    return longest_scenes\n",
    "\n",
    "\n",
    "def sort_scenes_by_frame(scenes_list):\n",
    "    '''Sort scenes by their start frame number.'''\n",
    "    sorted_scenes = sorted(scenes_list, key=lambda scene: scene[0].get_frames())\n",
    "    return sorted_scenes\n",
    "\n",
    "\n",
    "def get_num_grids(video_path):\n",
    "    '''Get number of grids to be created'''\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    duration = total_frames / fps\n",
    "\n",
    "    # Calculate number of grids based on the duration\n",
    "    duration = round(duration, 2)\n",
    "    if ((duration // 60) + 1) <= 5:\n",
    "        return int(((duration // 60) + 1))\n",
    "    else:\n",
    "        return 5\n",
    "        \n",
    "\n",
    "def extract_k_frames_from_scene(video_path, scene, k):\n",
    "    '''Extract k frames evenly spaced from each scene.'''\n",
    "    # Extract frame numbers from scene start and end\n",
    "    start_frame = scene[0].get_frames() + 1\n",
    "    end_frame = scene[1].get_frames() - 1\n",
    "\n",
    "    # Create k equally spaced frame indices within the scene's range\n",
    "    frame_indices = np.linspace(start_frame, end_frame, k, dtype=int)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    # Extract frames from calculated indices\n",
    "    for frame_no in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def create_image_grid(frames, grid_size=(1000, 1000)):\n",
    "    '''Arrange 6 frames into a 3x2 grid and resize to the specified grid size.'''\n",
    "    # Ensure all frames have the same size for concatenation\n",
    "    frames = [cv2.resize(frame, (640, 360)) for frame in frames]  # Resize to a common size like 640x360\n",
    "    rows = [np.concatenate(frames[i:i+2], axis=1) for i in range(0, 6, 2)]\n",
    "    image_grid = np.concatenate(rows, axis=0)\n",
    "    \n",
    "    return np.array(Image.fromarray(image_grid).resize(grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeaaa50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:51.854975Z",
     "iopub.status.busy": "2025-07-29T02:16:51.854578Z",
     "iopub.status.idle": "2025-07-29T02:16:51.861644Z",
     "shell.execute_reply": "2025-07-29T02:16:51.860493Z"
    },
    "papermill": {
     "duration": 0.077165,
     "end_time": "2025-07-29T02:16:51.863760",
     "exception": false,
     "start_time": "2025-07-29T02:16:51.786595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_images(video_path, n=6):\n",
    "    ''' 1. Detect scenes\n",
    "        2. Get k; where k = num_grids\n",
    "        3. Get the 6k longest scenes\n",
    "        4. Sort scenes wrt frame numbers\n",
    "        5. Extract 1 frame per 6k scene\n",
    "        6. Create k image grids of 6 frames each\n",
    "     '''\n",
    "    scene_list = detect_scenes(video_path)\n",
    "    k = get_num_grids(video_path)\n",
    "    longest_scenes = get_top_n_longest_scenes(scene_list, n*k)\n",
    "    scenes = sort_scenes_by_frame(longest_scenes)\n",
    "\n",
    "    frames = []\n",
    "    for scene in scenes:\n",
    "        frames.extend(extract_k_frames_from_scene(video_path, scene, 1))\n",
    "\n",
    "    grids = []\n",
    "    for i in range(k):\n",
    "        start_idx = i * n\n",
    "        end_idx = start_idx + n\n",
    "        grid_frames = frames[start_idx:end_idx]\n",
    "        grid = create_image_grid(grid_frames, grid_size=(1000, 1000))\n",
    "        grids.append(grid)\n",
    "\n",
    "    return grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6b569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:51.996372Z",
     "iopub.status.busy": "2025-07-29T02:16:51.995640Z",
     "iopub.status.idle": "2025-07-29T02:16:52.006596Z",
     "shell.execute_reply": "2025-07-29T02:16:52.005512Z"
    },
    "papermill": {
     "duration": 0.079155,
     "end_time": "2025-07-29T02:16:52.008641",
     "exception": false,
     "start_time": "2025-07-29T02:16:51.929486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_images_2(video_path, n=6):\n",
    "    ''' \n",
    "    Extracts image grids from video based on scenes.\n",
    "    Falls back to uniform sampling if no scenes are detected.\n",
    "    '''\n",
    "    # Step 1: Detect scenes\n",
    "    scene_list = detect_scenes(video_path)\n",
    "\n",
    "    # Step 2: Get number of grids and total frames needed\n",
    "    k = get_num_grids(video_path)\n",
    "    total_frames_needed = n * k\n",
    "\n",
    "    def extract_nk_frames(video_path, total_frames_needed):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "        if total_frames == 0:\n",
    "            cap.release()\n",
    "            return []\n",
    "    \n",
    "        # Get `total_frames_needed` evenly spaced frame indices\n",
    "        selected_indices = np.linspace(0, total_frames - 1, total_frames_needed, dtype=int)\n",
    "    \n",
    "        frames = []\n",
    "        for idx in selected_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            success, frame = cap.read()\n",
    "            if success:\n",
    "                frames.append(frame)\n",
    "    \n",
    "            if len(frames) >= total_frames_needed:\n",
    "                break\n",
    "    \n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    # Case 1: No scenes detected → use full video sampling\n",
    "    if not scene_list:\n",
    "        frames = extract_nk_frames(video_path, total_frames_needed)\n",
    "\n",
    "    # Case 2: Scenes detected → extract from each scene\n",
    "    else:\n",
    "        available_scenes = len(scene_list)\n",
    "        frames_per_scene = total_frames_needed // available_scenes\n",
    "        remaining_frames = total_frames_needed % available_scenes\n",
    "\n",
    "        if available_scenes == 1:\n",
    "            frames_per_scene = total_frames_needed\n",
    "            remaining_frames = 0\n",
    "\n",
    "        frames = []\n",
    "        for i, scene in enumerate(scene_list):\n",
    "            num_frames = frames_per_scene + (1 if i < remaining_frames else 0)\n",
    "            frames.extend(extract_k_frames_from_scene(video_path, scene, num_frames))\n",
    "\n",
    "        frames = frames[:total_frames_needed]\n",
    "\n",
    "    # Step 3: Ensure enough frames for all grids\n",
    "    if len(frames) < n:\n",
    "        frames = frames * (n // len(frames)) + frames[:(n % len(frames))]\n",
    "\n",
    "    # Step 4: Create image grids\n",
    "    grids = []\n",
    "    for i in range(k):\n",
    "        start_idx = i * n\n",
    "        end_idx = start_idx + n\n",
    "        grid_frames = frames[start_idx:end_idx]\n",
    "        if grid_frames:\n",
    "            grid = create_image_grid(grid_frames, grid_size=(1000, 1000))\n",
    "            grids.append(grid)\n",
    "\n",
    "    return grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b2e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:52.286363Z",
     "iopub.status.busy": "2025-07-29T02:16:52.285381Z",
     "iopub.status.idle": "2025-07-29T02:16:52.292172Z",
     "shell.execute_reply": "2025-07-29T02:16:52.290993Z"
    },
    "papermill": {
     "duration": 0.075567,
     "end_time": "2025-07-29T02:16:52.294285",
     "exception": false,
     "start_time": "2025-07-29T02:16:52.218718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(video_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd559b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:52.432014Z",
     "iopub.status.busy": "2025-07-29T02:16:52.431141Z",
     "iopub.status.idle": "2025-07-29T02:16:52.436031Z",
     "shell.execute_reply": "2025-07-29T02:16:52.434946Z"
    },
    "papermill": {
     "duration": 0.07703,
     "end_time": "2025-07-29T02:16:52.438286",
     "exception": false,
     "start_time": "2025-07-29T02:16:52.361256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai \n",
    "\n",
    "os.environ[\"API_KEY\"] = \"\"\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"]) \n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\", system_instruction=\"You are an expert content moderator.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df69d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:52.718028Z",
     "iopub.status.busy": "2025-07-29T02:16:52.717238Z",
     "iopub.status.idle": "2025-07-29T02:16:52.723247Z",
     "shell.execute_reply": "2025-07-29T02:16:52.722138Z"
    },
    "papermill": {
     "duration": 0.077981,
     "end_time": "2025-07-29T02:16:52.725787",
     "exception": false,
     "start_time": "2025-07-29T02:16:52.647806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9dbe5",
   "metadata": {
    "papermill": {
     "duration": 0.066065,
     "end_time": "2025-07-29T02:16:53.001542",
     "exception": false,
     "start_time": "2025-07-29T02:16:52.935477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b12d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:53.134264Z",
     "iopub.status.busy": "2025-07-29T02:16:53.133041Z",
     "iopub.status.idle": "2025-07-29T02:16:53.138230Z",
     "shell.execute_reply": "2025-07-29T02:16:53.137401Z"
    },
    "papermill": {
     "duration": 0.073707,
     "end_time": "2025-07-29T02:16:53.140289",
     "exception": false,
     "start_time": "2025-07-29T02:16:53.066582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LLM_Output(typing.TypedDict):\n",
    "    label: str\n",
    "    language: list[str]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8e95d",
   "metadata": {
    "papermill": {
     "duration": 0.065871,
     "end_time": "2025-07-29T02:16:53.271516",
     "exception": false,
     "start_time": "2025-07-29T02:16:53.205645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5b48d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:53.407891Z",
     "iopub.status.busy": "2025-07-29T02:16:53.407529Z",
     "iopub.status.idle": "2025-07-29T02:16:53.416473Z",
     "shell.execute_reply": "2025-07-29T02:16:53.415451Z"
    },
    "papermill": {
     "duration": 0.078067,
     "end_time": "2025-07-29T02:16:53.418422",
     "exception": false,
     "start_time": "2025-07-29T02:16:53.340355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt =  \"\"\"\"A video can be considered inappropriate for children if it contains physical violence (cartoonish, realistic, or gory), interpersonal violence (bullying, pranks, meanness, belittling, controlling behavior, talking down to others, or manipulation), self-harm or suicide (depictions of harm inflicted on oneself or suicidal thoughts/tendencies), extreme stunts (life-endangering, high-risk activities/challenges that require adult supervision), dangerous products or services (like paintball, airsoft, fireworks, weapons, hunting equipment, graffiti products), scary content (horror, suspense, loud violence, zombies, skeletons, masks, scary clowns, blood, dangerous fire, car crashes, medical procedures or other scary visuals), sexual content (innuendos, sexual behavior, nudity, suggestiveness), intimacy (romantic conduct, couples expressing love, sensual contact, dating, relationships or any expression of love or loss for a significant other,  lyrics about or mentions of love or romance, heartbreak), inappropriate clothing (significant skin exposure or sheer clothing outside of appropriate contexts such as sunbathing, swinmwear,etc), substance use (mention, depictions, or use of alcohol, drugs, tobacco), gambling (betting, gambling, casinos, online or real world casino games, online trading), offensive language (from mild to severe with words such as “stupid” or “idiot”), stereotypes (racial, ethnic, gender, body shaming), crude humor (inappropriate humor, innuendos), weapons (guns, swords, knives, etc but toy guns such as nerf guns are fine), unsuitable media (ads for violent or adult-themed video games, casino games, games with romance or battle or first person-shooter games), fight sports (boxing, wrestling, martial arts), financial content (difference, rolling spot forex, financial spread betting and synonymous products but banking ads are fine), health and beauty services (cosmetic procedures, weight loss, tanning, piercings, tattoos, cosmetics and makeup (no matter their essential benefits) and other products related to external personal care focused on body image but ads on fashion products and essential hygiene products like sunscreen, shampoo, toothpaste, etc. are fine, however, products like sanitary care and hair removal is inappropriate), ads or sponsored content for consumable food or drink products (regardless of nutritional content), where the product is consumed by a person or shown with branding, packaging, or promotional intent are inappropriate; general content like cooking shows, recipes, or scenes where food is simply present or being eaten without promotion are acceptable, pet/animal food products and healthcare, healthcare or medical topics (reproductive health, mental health, substance abuse, eating disorders, pharmaceuticals, health insurance, any nutritional or dietary supplements), sensitive topics (politics, religion, controversial social issues, astrology, occult, paranormal, religious sermons), privacy or safety risks (services or competitions requiring personal information, social networks for strangers, spy services, or video game skin trading), or any explicit depictions or discussions of death or dying. Consider that you are an expert content moderator. Based on the image grids provided containing frames from key scenes from within the video and its audio, is the video advertisement inappropriate for children on this definition? If it is, label it as “inappropriate.” If it is not, label it as “appropriate.” Justify your conclusion. Also, mention what language is being spoken if there is any. \n",
    "Make the outputs in JSON format: {“label”: “appropriate” or “inappropriate”, “justification”: “brief explanation of the classification”, “language”: “language detected in the video”}.\"\"\"\n",
    "\n",
    "prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10101617",
   "metadata": {
    "id": "w8esg7q8hSTW",
    "papermill": {
     "duration": 0.064968,
     "end_time": "2025-07-29T02:16:53.548981",
     "exception": false,
     "start_time": "2025-07-29T02:16:53.484013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4271ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:53.681043Z",
     "iopub.status.busy": "2025-07-29T02:16:53.680654Z",
     "iopub.status.idle": "2025-07-29T02:16:53.687145Z",
     "shell.execute_reply": "2025-07-29T02:16:53.686102Z"
    },
    "papermill": {
     "duration": 0.074885,
     "end_time": "2025-07-29T02:16:53.689038",
     "exception": false,
     "start_time": "2025-07-29T02:16:53.614153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39202e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:53.821834Z",
     "iopub.status.busy": "2025-07-29T02:16:53.821431Z",
     "iopub.status.idle": "2025-07-29T02:16:53.825807Z",
     "shell.execute_reply": "2025-07-29T02:16:53.824838Z"
    },
    "papermill": {
     "duration": 0.073565,
     "end_time": "2025-07-29T02:16:53.827770",
     "exception": false,
     "start_time": "2025-07-29T02:16:53.754205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = len(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291210a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T02:16:53.961559Z",
     "iopub.status.busy": "2025-07-29T02:16:53.961031Z",
     "iopub.status.idle": "2025-07-29T04:29:46.037171Z",
     "shell.execute_reply": "2025-07-29T04:29:46.035876Z"
    },
    "id": "cSoT7-9DDknR",
    "papermill": {
     "duration": 7972.950961,
     "end_time": "2025-07-29T04:29:46.845402",
     "exception": false,
     "start_time": "2025-07-29T02:16:53.894441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "predicted_labels = []\n",
    "languages = []\n",
    "responses = []\n",
    "ground_truths = []\n",
    "remaining = []\n",
    "key_count = 0\n",
    "\n",
    "img_dir = '/kaggle/working/Images'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)\n",
    "    \n",
    "for i in range(start,end): \n",
    "    print('ID: ', video_ids[i])\n",
    "\n",
    "    if video_ids[i] in available_ids:\n",
    "\n",
    "        try:\n",
    "            contents_of_ad = os.listdir('/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i]) \n",
    "            contents_of_ad.remove('audio.mp3') \n",
    "            video_path = '/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i] + '/' + contents_of_ad[0] \n",
    "            audio_path = f'/kaggle/input/youtube-data/Ads/Ads/{video_ids[i]}/audio.mp3'\n",
    "            try:\n",
    "                audio_file = genai.upload_file(path=audio_path, resumable=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading audio: {e}\")\n",
    "                remaining.append(video_ids[i])\n",
    "                continue\n",
    "            # Extract multiple images representative of the video\n",
    "            try:\n",
    "                images = get_images(video_path)\n",
    "            except:\n",
    "                images = get_images_2(video_path)\n",
    "                \n",
    "            # Save each image returned by extract_images_of_frames\n",
    "            image_paths = []\n",
    "            for idx, img in enumerate(images):\n",
    "                # Convert NumPy array to PIL image\n",
    "                image = Image.fromarray(img)\n",
    "                \n",
    "                # Save the image to a file\n",
    "                image_name = f\"{video_ids[i]}_{idx + 1}.png\"\n",
    "                image_path = os.path.join(img_dir, image_name)\n",
    "                image.save(image_path)\n",
    "                image_paths.append(image_path)\n",
    "                print(image_path)\n",
    "                \n",
    "            # Display the images\n",
    "            fig, axes = plt.subplots(1, len(images), figsize=(10, 3))\n",
    "            if len(images) == 1:\n",
    "                axes.imshow(images[0])\n",
    "                axes.axis('off')\n",
    "            else:\n",
    "                for ax, img in zip(axes, images):\n",
    "                    ax.imshow(img)\n",
    "                    ax.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Upload images and handle potential errors\n",
    "            uploaded_files = []\n",
    "            try:\n",
    "                for image_path in image_paths:\n",
    "                    uploaded_file = genai.upload_file(path=image_path, resumable=False)\n",
    "                    uploaded_files.append(uploaded_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading images: {e}\")\n",
    "                remaining.append(video_ids[i])\n",
    "                continue\n",
    "                \n",
    "            # Check if all images have uploaded\n",
    "            try:\n",
    "                for uploaded_file in uploaded_files:\n",
    "                    while uploaded_file.state.name == \"PROCESSING\":\n",
    "                        print('.', end='')\n",
    "                        time.sleep(10)\n",
    "                        uploaded_file = genai.get_file(uploaded_file.name)\n",
    "                    if uploaded_file.state.name == \"FAILED\":\n",
    "                        raise ValueError(uploaded_file.state.name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during image processing: {e}\")\n",
    "                remaining.append(video_ids[i])\n",
    "                continue\n",
    "\n",
    "            # Make inference\n",
    "            try:\n",
    "                inputs_ = [audio_file] \n",
    "                inputs_.extend(uploaded_files) \n",
    "                inputs_.extend([prompt]) \n",
    "                response = model.generate_content(inputs_,\n",
    "                                                  generation_config=genai.GenerationConfig(\n",
    "                                                      response_mime_type=\"application/json\",\n",
    "                                                      temperature=0,  # Set temperature here \n",
    "                                                      response_schema=LLM_Output),\n",
    "                                                  safety_settings={\n",
    "                                                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                                                  })\n",
    "            except Exception as e:\n",
    "                print(f\"Error making inference: {e}\")\n",
    "                remaining.append(video_ids[i])\n",
    "                continue\n",
    "\n",
    "            # Wrap response.text access in try-except\n",
    "            try:\n",
    "                print(\"Completed for video number:\", i, \"\\t\", video_ids[i])\n",
    "\n",
    "                dictionary = loads(response.text)\n",
    "                print(dictionary)\n",
    "\n",
    "                ids.append(video_ids[i])\n",
    "                predicted_labels.append(dictionary['label'])\n",
    "                languages.append(dictionary['language'])\n",
    "                responses.append(dictionary['response'])\n",
    "                ground_truths.append(primary_labels[i]) \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing response.text: {e}\")\n",
    "                remaining.append(video_ids[i])\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            remaining.append(video_ids[i])\n",
    "            continue\n",
    "\n",
    "        # Wait a bit to avoid exceeding rate limits\n",
    "        time.sleep(30)\n",
    "\n",
    "# At the end, print remaining videos\n",
    "print(\"Remaining videos with errors:\", remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988dc35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T04:29:48.550895Z",
     "iopub.status.busy": "2025-07-29T04:29:48.550295Z",
     "iopub.status.idle": "2025-07-29T04:29:48.556181Z",
     "shell.execute_reply": "2025-07-29T04:29:48.555115Z"
    },
    "papermill": {
     "duration": 0.866606,
     "end_time": "2025-07-29T04:29:48.558240",
     "exception": false,
     "start_time": "2025-07-29T04:29:47.691634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dir = '/kaggle/working/results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804ae58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T04:29:50.424783Z",
     "iopub.status.busy": "2025-07-29T04:29:50.424370Z",
     "iopub.status.idle": "2025-07-29T04:29:50.439488Z",
     "shell.execute_reply": "2025-07-29T04:29:50.438336Z"
    },
    "papermill": {
     "duration": 0.851711,
     "end_time": "2025-07-29T04:29:50.442747",
     "exception": false,
     "start_time": "2025-07-29T04:29:49.591036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# At the end, print remaining videos \n",
    "remaining_df = pd.DataFrame({'Video Id': remaining})\n",
    "\n",
    "# Save to CSV\n",
    "remaining_df.to_csv(f'/kaggle/working/results/DAVSP-gemini-audio-remaining-{start}-{end}.csv', index=False)\n",
    "\n",
    "print(\"Remaining videos saved to remaining.csv\")\n",
    "print(\"Remaining videos with errors:\", remaining) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bb066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T04:29:52.191326Z",
     "iopub.status.busy": "2025-07-29T04:29:52.190819Z",
     "iopub.status.idle": "2025-07-29T04:29:52.208615Z",
     "shell.execute_reply": "2025-07-29T04:29:52.207258Z"
    },
    "id": "6JsiW7gbvxRA",
    "papermill": {
     "duration": 0.909885,
     "end_time": "2025-07-29T04:29:52.210877",
     "exception": false,
     "start_time": "2025-07-29T04:29:51.300992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame({\n",
    "    'Video Id': ids,\n",
    "    'Primary Label': ground_truths,\n",
    "    'Predicted Label': predicted_labels,\n",
    "    'Response': responses, \n",
    "    'Languages': languages \n",
    "})\n",
    "\n",
    "new_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2cb9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T04:29:54.198189Z",
     "iopub.status.busy": "2025-07-29T04:29:54.197747Z",
     "iopub.status.idle": "2025-07-29T04:29:54.207009Z",
     "shell.execute_reply": "2025-07-29T04:29:54.205836Z"
    },
    "id": "Tv1_5sfJwRfu",
    "papermill": {
     "duration": 1.072029,
     "end_time": "2025-07-29T04:29:54.209359",
     "exception": false,
     "start_time": "2025-07-29T04:29:53.137330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.to_csv(f'/kaggle/working/results/davsp_gemini-audio-{start}-{end}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6449f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T04:29:56.239884Z",
     "iopub.status.busy": "2025-07-29T04:29:56.239473Z",
     "iopub.status.idle": "2025-07-29T04:29:56.246300Z",
     "shell.execute_reply": "2025-07-29T04:29:56.245000Z"
    },
    "id": "4hsLj8P8Mg-e",
    "papermill": {
     "duration": 1.01304,
     "end_time": "2025-07-29T04:29:56.248590",
     "exception": false,
     "start_time": "2025-07-29T04:29:55.235550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Changing to binary lists \n",
    "\n",
    "predictions = [1 if pred == 'inappropriate' else 0 for pred in predicted_labels] \n",
    "ground_truths = [1 if label == 'inappropriate' else 0 for label in ground_truths] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c5a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T04:29:58.437979Z",
     "iopub.status.busy": "2025-07-29T04:29:58.437569Z",
     "iopub.status.idle": "2025-07-29T04:29:58.768160Z",
     "shell.execute_reply": "2025-07-29T04:29:58.766776Z"
    },
    "papermill": {
     "duration": 1.455208,
     "end_time": "2025-07-29T04:29:58.770371",
     "exception": false,
     "start_time": "2025-07-29T04:29:57.315163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtaining classification report \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "report = classification_report(ground_truths, predictions) \n",
    "print(report) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c2c1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T04:30:00.378228Z",
     "iopub.status.busy": "2025-07-29T04:30:00.377779Z",
     "iopub.status.idle": "2025-07-29T04:30:02.114845Z",
     "shell.execute_reply": "2025-07-29T04:30:02.113628Z"
    },
    "papermill": {
     "duration": 2.585395,
     "end_time": "2025-07-29T04:30:02.117465",
     "exception": false,
     "start_time": "2025-07-29T04:29:59.532070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(ground_truths, predictions)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Appropriate', 'Inapproriate'], yticklabels=['Appropriate', 'Inapproriate'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6395471,
     "sourceId": 10328861,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6296913,
     "sourceId": 10349036,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7116738,
     "sourceId": 12294403,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6411015,
     "sourceId": 12332992,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5987034,
     "sourceId": 12343434,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5684885,
     "sourceId": 12532117,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7809201,
     "sourceId": 12541620,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7753845,
     "sourceId": 12589272,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7951184,
     "sourceId": 12589506,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8278.882399,
   "end_time": "2025-07-29T04:30:09.642404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-29T02:12:10.760005",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
